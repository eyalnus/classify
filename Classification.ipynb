{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install numpy\n!pip install matplotlib\n!pip install seaborn\n!pip install pandas\n!pip install sklearn\n!pip install scipy\n!pip install pydotplus\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.naive_bayes import GaussianNB\nimport datetime\nfrom pandas.api.types import is_numeric_dtype\nimport sklearn as skl\nfrom scipy.stats import skewnorm\nimport scipy.stats as stats\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nimport pydotplus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification using sklearn\nScikit-learn (sklearn) is a Python library dedicated to machine learning. It contains classifier and regression algorithm objects which implement an API for training and predicting models. Additionally, it contains some methods for data manipulation and performance metric measuring of predictrive models."},{"metadata":{},"cell_type":"markdown","source":"### Quick example - Iris dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris = sns.load_dataset('iris') #seaborn has some built in datasets\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(iris, hue='species', height=1.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Splitting the data set into feature vector X and target variable y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_iris = iris.drop('species', axis=1)\nprint(X_iris.shape)\ny_iris = iris['species']\nprint(y_iris.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Splitting the data set into training and test sets. By default, test set size is 25% of data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split \nXtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\n                                                random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Training and predicting using Naive Bayes classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.naive_bayes import GaussianNB # 1. choose model class\nmodel = GaussianNB()                       # 2. instantiate model\nmodel.fit(Xtrain, ytrain)                  # 3. fit model to data\ny_model = model.predict(Xtest)             # 4. predict on new data (output is numpy array)\n\nypred = pd.Series(y_model,name=\"prediction\")\npredicted = pd.concat([Xtest.reset_index(),ytest.reset_index(),ypred],axis=1)\npredicted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Calculate the accuracy as an average of accuracy per class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn import metrics\nmetrics.accuracy_score(ytest, y_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### What happens if we select less columns?"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_iris = iris.drop(['species','petal_length','petal_width'], axis=1)\ny_iris = iris['species']\nXtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\n                                                random_state=1)\nmodel = GaussianNB()                       # 2. instantiate model\nmodel.fit(Xtrain, ytrain)                  # 3. fit model to data\ny_model = model.predict(Xtest)             # 4. predict on new data (output is numpy array)\n\nypred = pd.Series(y_model,name=\"prediction\")\npredicted = pd.concat([Xtest.reset_index(),ytest.reset_index(),ypred],axis=1)\nprint(metrics.accuracy_score(ytest, y_model))\n\npredicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_iris = iris.drop(['species','sepal_length','sepal_width'], axis=1)\ny_iris = iris['species']\nXtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\n                                                random_state=1)\nmodel = GaussianNB()                       # 2. instantiate model\nmodel.fit(Xtrain, ytrain)                  # 3. fit model to data\ny_model = model.predict(Xtest)             # 4. predict on new data (output is numpy array)\n\nypred = pd.Series(y_model,name=\"prediction\")\npredicted = pd.concat([Xtest.reset_index(),ytest.reset_index(),ypred],axis=1)\nprint(metrics.accuracy_score(ytest, y_model))\npredicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayes_plot(df,model=\"gnb\",spread=30):\n    df.dropna()\n    colors = 'seismic'\n    col1 = df.columns[0]\n    col2 = df.columns[1]\n    target = df.columns[2]\n    sns.scatterplot(data=df, x=col1, y=col2,hue=target)\n    plt.show()\n    y = df[target]  # Target variable\n    X = df.drop(target, axis=1)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)  # 70% training and 30% test\n\n    clf = GaussianNB()\n    if (model != \"gnb\"):\n        clf = DecisionTreeClassifier(max_depth=model)\n    clf = clf.fit(X_train, y_train)\n    \n    # Train Classifer\n    \n\n    prob = len(clf.classes_) == 2\n\n    # Predict the response for test dataset\n\n    y_pred = clf.predict(X_test)\n    print(metrics.classification_report(y_test, y_pred))\n\n    hueorder = clf.classes_\n    def numify(val):\n        return np.where(clf.classes_ == val)[0]\n\n    Y = y.apply(numify)\n    x_min, x_max = X.loc[:, col1].min() - 1, X.loc[:, col1].max() + 1\n    y_min, y_max = X.loc[:, col2].min() - 1, X.loc[:, col2].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2),\n                         np.arange(y_min, y_max, 0.2))\n\n    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    if prob:\n\n        Z = Z[:,1]-Z[:,0]\n    else:\n        colors = \"Set1\"\n        Z = np.argmax(Z, axis=1)\n\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=colors, alpha=0.5)\n    plt.colorbar()\n    if not prob:\n        plt.clim(0,len(clf.classes_)+3)\n    sns.scatterplot(data=df[::spread], x=col1, y=col2, hue=target, hue_order=hueorder,palette=colors)\n    fig = plt.gcf()\n    fig.set_size_inches(12, 8)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_plot(pd.concat([X_iris,y_iris],axis=1),spread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_poor = iris.drop(['petal_length','petal_width'], axis=1)\nbayes_plot(iris_poor,spread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_plot(pd.concat([X_iris,y_iris],axis=1),model=4,spread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_plot(pd.concat([X_iris,y_iris],axis=1),model=2,spread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.0)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blob_df = pd.DataFrame(X,columns=['X','Y'])\nblob_series = pd.Series(y,name=\"target\")\nprint(pd.concat([blob_df,blob_series],axis=1))\nbayes_plot(pd.concat([blob_df,blob_series],axis=1),model=3,spread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_plot(pd.concat([blob_df,blob_series],axis=1),model=6,spread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from io import StringIO\n#!pip install graphviz\n!conda install -y python-graphviz\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\n\ntech = pd.read_csv('dataset-tortuga.csv')\ntech.dropna(inplace=True)\nX = tech.drop([\"PROFILE\", \"NAME\",\"USER_ID\",\"Unnamed: 0\"],axis=1)\ny = tech['PROFILE']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier()\nclf.fit(X_train,y_train)\n\n#importance = zip(X.columns, clf.feature_importances_)\nresult = permutation_importance(clf, X, y, n_repeats=10,random_state=0)\nimportance = zip(X.columns,result['importances_mean'])\n# summarize feature importance\nfor i,v in importance:\n    print('Feature: %s, Score: %.5f' % (i,v))\n# plot feature importance\nprint(len(X.columns),[x[1] for x in importance])\nplt.bar(range(len(X.columns)), result['importances_mean'])\nplt.xticks(ticks=range(len(X.columns)),labels=X.columns, rotation=90)\nplt.show()\n\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#highest importance\nX_2d = X[[\"NUM_COURSES_BEGINNER_BACKEND\",\"AVG_SCORE_DATASCIENCE\"]]\ny_2d = tech['PROFILE']\nX_train, X_test, y_train, y_test = train_test_split(X_2d, y_2d, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X_2d.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech2d.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Note: this does not mean that the top \"important\" features will always yield the best results. Remember that some split decisions are random and will affect importance.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_plot(pd.concat([X_2d,y_2d],axis=1),model=20,spread=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lowest importance \nX_2d = X[[\"NUM_COURSES_ADVANCED_DATASCIENCE\",\"NUM_COURSES_ADVANCED_FRONTEND\"]]\ny_2d = tech['PROFILE']\nX_train, X_test, y_train, y_test = train_test_split(X_2d, y_2d, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier(max_depth=4)\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech_depth4.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier(max_depth=6)\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech_depth6.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier(min_impurity_decrease=0.003)\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech_impurity.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier(max_leaf_nodes =50)\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech_maxleaf.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier(min_samples_split=0.01)\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech_minsamplesplit.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cut = X.drop([\"NUM_COURSES_ADVANCED_DATASCIENCE\",\"NUM_COURSES_ADVANCED_FRONTEND\"],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X_cut, y, test_size=0.3,random_state=1)  # 70% training and 30% test\nclf = DecisionTreeClassifier(min_samples_split=0.01)\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_pred))\n\ndot_data=StringIO()\nexport_graphviz(clf,out_file=dot_data,filled=True,rounded=True,feature_names=X_cut.columns,class_names=clf.classes_)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Tech_minsamplesplit2.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}